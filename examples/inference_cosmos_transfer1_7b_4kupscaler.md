# Transfer Inference featuring 4k Upscaler

## Install Cosmos-Transfer1

### Environment setup

Please refer to the Inference section of [INSTALL.md](/INSTALL.md#inference) for instructions on environment setup.

### Download Checkpoints

1. Generate a [Hugging Face](https://huggingface.co/settings/tokens) access token. Set the access token to 'Read' permission (default is 'Fine-grained').

2. Log in to Hugging Face with the access token:

```bash
huggingface-cli login
```

3. Accept the [LlamaGuard-7b terms](https://huggingface.co/meta-llama/LlamaGuard-7b)

4. Download the Cosmos model weights from [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-transfer1-67c9d328196453be6e568d3e):

```bash
CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) python scripts/download_checkpoints.py --output_dir checkpoints/
```

Note that this will require about 300GB of free storage. Not all these checkpoints will be used in every generation.

5. The downloaded files should be in the following structure:

```
checkpoints/
├── nvidia
│   │
│   ├── Aegis-AI-Content-Safety-LlamaGuard-Defensive-1.0
│   │   ├── README.md
│   │   ├── adapter_config.json
│   │   ├── adapter_model.safetensors
│   │   └── models--nvidia--Aegis-AI-Content-Safety-LlamaGuard-Defensive-1.0/...
│   │
│   ├── Cosmos-Guardrail1
│   │   ├── README.md
│   │   ├── blocklist/...
│   │   ├── face_blur_filter/...
│   │   └── video_content_safety_filter/...
│   │
│   ├── Cosmos-Transfer1-7B
│   │   ├── base_model.pt
│   │   ├── vis_control.pt
│   │   ├── edge_control.pt
│   │   ├── seg_control.pt
│   │   ├── depth_control.pt
│   │   ├── 4kupscaler_control.pt
│   │   └── config.json
│   │
│   ├── Cosmos-Transfer1-7B-Sample-AV/
│   │   ├── base_model.pt
│   │   ├── hdmap_control.pt
│   │   └── lidar_control.pt
│   │
│   └── Cosmos-Tokenize1-CV8x8x8-720p
│       ├── decoder.jit
│       ├── encoder.jit
│       ├── autoencoder.jit
│       └── mean_std.pt
│
├── depth-anything/...
├── facebook/...
├── google-t5/...
├── IDEA-Research/...
└── meta-llama/...
```

## Run Example

For a general overview of how to use the model see [this guide](/examples/inference_cosmos_transfer1_7b.md).


Ensure you are at the root of the repository before executing the following:

```bash
export CUDA_VISIBLE_DEVICES=0
export CHECKPOINT_DIR="${CHECKPOINT_DIR:=./checkpoints}"
CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) python cosmos_transfer1/diffusion/inference/transfer.py \
    --checkpoint_dir $CHECKPOINT_DIR \
    --video_save_folder outputs/inference_upscaler \
    --controlnet_specs assets/inference_upscaler.json \
    --num_steps 10 \
    --offload_text_encoder_model
```

You can also choose to run the inference on multiple GPUs as follows:

```bash
export CUDA_VISIBLE_DEVICES="${CUDA_VISIBLE_DEVICES:=0,1,2,3}"
export CHECKPOINT_DIR="${CHECKPOINT_DIR:=./checkpoints}"
export NUM_GPU="${NUM_GPU:=4}"
CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) torchrun --nproc_per_node=$NUM_GPU --nnodes=1 --node_rank=0 cosmos_transfer1/diffusion/inference/transfer.py \
    --checkpoint_dir $CHECKPOINT_DIR \
    --video_save_folder outputs/inference_upscaler \
    --controlnet_specs assets/inference_upscaler.json \
    --num_steps 10 \
    --offload_text_encoder_model \
    --num_gpus $NUM_GPU
```

This launches `transfer.py` and configures the controlnets for inference according to `assets/inference_upscaler.json`:

```json
{
    "input_video_path" : "assets/inference_upscaler_input_video.mp4",
    "upscale": {
        "control_weight": 0.5
    },
}
```

### Explanation of the controlnet spec

* `prompt` (optional) specifies the prompt for the upscaler. If no prompt is provided, a default prompt saying the video is high-quality is used.
* `input_video_path` specifies the input video
* `sigma_max` specifies the level of noise that should be added to the input video before feeding through the base model branch
* The `control_weight` parameter is a number within the range [0, 1] that controls how strongly the controlnet branch should affect the output of the model. The larger the value (closer to 1.0), the more strongly the generated video will adhere to the controlnet input. However, this rididity may come at a cost of quality. Lower (closer to 0) values would give more creative liberty to the model at the cost of reduced adherance. Usually a middleground value, say 0.5, yields optinal results.

### The input and output videos

The input video is a 1280 x 704 video generated by Cosmos-Predict1-7B-Text2World:

<video src="https://github.com/user-attachments/assets/45576ddf-d595-4d54-9b97-a040671937c9">
  Your browser does not support the video tag.
</video>

Here is what the model outputs, a high-resolution 3840 x 2112 video:

<video src="https://github.com/user-attachments/assets/768b60d7-9dc4-4aed-9297-f634b5e34981">
  Your browser does not support the video tag.
</video>
